// @COPYRIGHT@
// Licensed under MIT license.
// See LICENSE.TXT file in the project root for more information.
// ==============================================================
#ifndef __SHADOW_MEMORY__
#define __SHADOW_MEMORY__
#include <stdint.h>
#include <sys/mman.h>

#if __cplusplus > 199711L
#    include <atomic>
#    include <tuple>
#endif // end  __cplusplus > 199711L

#define SHADOW_MEMORY_TEST 0

// 64KB shadow pages
#define PAGE_OFFSET_BITS (16LL)
#define PAGE_OFFSET(addr) (addr & 0xFFFF)
#define PAGE_OFFSET_MASK (0xFFFF)
#define SHADOW_PAGE_SIZE (1 << PAGE_OFFSET_BITS)

// 2 level page table
#define PTR_SIZE (sizeof(void *))

#define LEVEL_1_PAGE_TABLE_BITS (20)
#define LEVEL_1_PAGE_TABLE_ENTRIES (1 << LEVEL_1_PAGE_TABLE_BITS)
#define LEVEL_1_PAGE_TABLE_SIZE (LEVEL_1_PAGE_TABLE_ENTRIES * PTR_SIZE)
// LEVEL_1_PAGE_TABLE_SIZE = 1 << 20 * sizeof(void *)

#define LEVEL_2_PAGE_TABLE_BITS (12)
#define LEVEL_2_PAGE_TABLE_ENTRIES (1 << LEVEL_2_PAGE_TABLE_BITS)
#define LEVEL_2_PAGE_TABLE_SIZE (LEVEL_2_PAGE_TABLE_ENTRIES * PTR_SIZE)

#define LEVEL_1_PAGE_TABLE_SLOT(addr) \
    ((((uint64_t)addr) >> (LEVEL_2_PAGE_TABLE_BITS + PAGE_OFFSET_BITS)) & 0xfffff)
#define LEVEL_2_PAGE_TABLE_SLOT(addr) ((((uint64_t)addr) >> (PAGE_OFFSET_BITS)) & 0xFFF)

using namespace std;

/*
template <typename... Ts>
struct ShadowType {
    tuple<Ts[SHADOW_PAGE_SIZE]...> f;
    void * operator new(size_t sz) {
        void * p = mmap(0, sz, PROT_WRITE | PROT_READ,  MAP_PRIVATE | MAP_ANONYMOUS, 0,
0); if(p == MAP_FAILED) { perror("mmap l2pg"); exit(-1);
        }
        return p;
    }
    void operator delete(void *p) {
        munmap(p, sizeof(ShadowType<Ts...>));
    }
};
*/

#if __cplusplus > 199711L
template <class... Args> using ShadowTuple = std::tuple<Args[SHADOW_PAGE_SIZE]...>;

template <typename... Ts> class ConcurrentShadowMemory {
    // All fwd declarations
    atomic<atomic<ShadowTuple<Ts...> *> *> *pageDirectory;
    // Given a address generated by the program, returns the corresponding shadow address
    // FLOORED to  SHADOW_PAGE_SIZE If the shadow page does not exist a new one is MMAPed
public:
    inline ConcurrentShadowMemory()
    {
        pageDirectory = (atomic<atomic<ShadowTuple<Ts...> *> *> *)mmap(
            0, LEVEL_1_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
            MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
        if (pageDirectory == MAP_FAILED) {
            perror("mmap pageDirectory");
            exit(-1);
        }
    }

    inline ~ConcurrentShadowMemory()
    {
        for (uint64_t i = 0; i < LEVEL_1_PAGE_TABLE_ENTRIES; i++) {
            atomic<ShadowTuple<Ts...> *> *l1Page;
            if ((l1Page = pageDirectory[i].load(memory_order_relaxed)) != 0) {
                for (uint64_t j = 0; j < LEVEL_2_PAGE_TABLE_ENTRIES; j++) {
                    ShadowTuple<Ts...> *l2Page;
                    if ((l2Page = l1Page[j].load(memory_order_relaxed)) != 0) {
                        delete l2Page;
                    }
                }
                if (0 != munmap(l1Page, LEVEL_2_PAGE_TABLE_SIZE)) {
                    perror("munmap pageDirectory");
                    exit(-1);
                }
            }
        }
        if (0 != munmap(pageDirectory, LEVEL_1_PAGE_TABLE_SIZE)) {
            perror("munmap pageDirectory");
            exit(-1);
        }
    }

    inline ShadowTuple<Ts...> *
    GetOrCreateShadowBaseAddress(const size_t address)
    {
        atomic<atomic<ShadowTuple<Ts...> *> *> *l1Ptr =
            &pageDirectory[LEVEL_1_PAGE_TABLE_SLOT(address)];
        atomic<ShadowTuple<Ts...> *> *v1;
        if ((v1 = l1Ptr->load(memory_order_consume)) == 0) {
            atomic<ShadowTuple<Ts...> *> *l1pg = (atomic<ShadowTuple<Ts...> *> *)mmap(
                0, LEVEL_2_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
                MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
            if (l1pg == MAP_FAILED) {
                perror("mmap l1pg");
                exit(-1);
            }

            atomic<ShadowTuple<Ts...> *> *nullVal = 0;
            if (!l1Ptr->compare_exchange_strong(nullVal, l1pg, memory_order_acq_rel,
                                                memory_order_relaxed)) {
                free(l1pg);
                v1 = l1Ptr->load(memory_order_consume);
            } else {
                v1 = l1pg;
            }
        }
        atomic<ShadowTuple<Ts...> *> *l2Ptr = &v1[LEVEL_2_PAGE_TABLE_SLOT(address)];
        ShadowTuple<Ts...> *v2;
        if ((v2 = l2Ptr->load(memory_order_consume)) == 0) {
            ShadowTuple<Ts...> *l2pg = new ShadowTuple<Ts...>;
            if (l2pg == MAP_FAILED) {
                perror("mmap l2pg");
                exit(-1);
            }
            ShadowTuple<Ts...> *nullVal = 0;
            if (!l2Ptr->compare_exchange_strong(nullVal, l2pg, memory_order_acq_rel,
                                                memory_order_relaxed)) {
                delete l2pg;
                v2 = l2Ptr->load(memory_order_consume);
            } else {
                v2 = l2pg;
            }
        }
        return v2;
    }
};

template <int I, typename... Ts>
inline __attribute__((always_inline)) typename std::tuple_element<I, tuple<Ts...>>::type *
GetOrCreateShadowAddress(ConcurrentShadowMemory<Ts...> &sm, const size_t address)
{
    ShadowTuple<Ts...> * shadowPage = sm.GetOrCreateShadowBaseAddress(address);
    return &(get<I>(*shadowPage)[PAGE_OFFSET((uint64_t)address)]);
}

template <typename... Ts> class ShadowMemory {
    // All fwd declarations
    ShadowTuple<Ts...> ***pageDirectory;
    // Given a address generated by the program, returns the corresponding shadow address
    // FLOORED to  SHADOW_PAGE_SIZE If the shadow page does not exist a new one is MMAPed
public:
    inline ShadowMemory()
    {
        pageDirectory = (ShadowTuple<Ts...> ***)mmap(0, LEVEL_1_PAGE_TABLE_SIZE,
                                                     PROT_WRITE | PROT_READ,
                                                     MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
        if (pageDirectory == MAP_FAILED) {
            perror("mmap pageDirectory");
            exit(-1);
        }
    }

    inline ~ShadowMemory()
    {
        for (uint64_t i = 0; i < LEVEL_1_PAGE_TABLE_ENTRIES; i++) {
            ShadowTuple<Ts...> **l1Page;
            if ((l1Page = pageDirectory[i]) != 0) {
                for (uint64_t j = 0; j < LEVEL_2_PAGE_TABLE_ENTRIES; j++) {
                    ShadowTuple<Ts...> *l2Page;
                    if ((l2Page = l1Page[j]) != 0) {
                        delete l2Page;
                    }
                }
                if (0 != munmap(l1Page, LEVEL_2_PAGE_TABLE_SIZE)) {
                    perror("munmap pageDirectory");
                    exit(-1);
                }
            }
        }
        if (0 != munmap(pageDirectory, LEVEL_1_PAGE_TABLE_SIZE)) {
            perror("munmap pageDirectory");
            exit(-1);
        }
    }

    inline ShadowTuple<Ts...> *
    GetOrCreateShadowBaseAddress(const size_t address)
    {
        ShadowTuple<Ts...> ***l1Ptr = &pageDirectory[LEVEL_1_PAGE_TABLE_SLOT(address)];
        ShadowTuple<Ts...> **v1;
        if ((v1 = *l1Ptr) == 0) {
            ShadowTuple<Ts...> **l1pg = (ShadowTuple<Ts...> **)mmap(
                0, LEVEL_2_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
                MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
            if (l1pg == MAP_FAILED) {
                perror("mmap l1pg");
                exit(-1);
            }
            *l1Ptr = l1pg;
            v1 = l1pg;
        }
        ShadowTuple<Ts...> **l2Ptr = &v1[LEVEL_2_PAGE_TABLE_SLOT(address)];
        ShadowTuple<Ts...> *v2;
        if ((v2 = *l2Ptr) == 0) {
            ShadowTuple<Ts...> *l2pg = new ShadowTuple<Ts...>;
            if (l2pg == MAP_FAILED) {
                perror("mmap l2pg");
                exit(-1);
            }
            *l2Ptr = l2pg;
            v2 = l2pg;
        }
        return v2;
    }
};

template <int I, typename... Ts>
inline __attribute__((always_inline)) typename std::tuple_element<I, tuple<Ts...>>::type *
GetOrCreateShadowAddress(ShadowMemory<Ts...> &sm, const size_t address)
{
    ShadowTuple<Ts...> * shadowPage = sm.GetOrCreateShadowBaseAddress(address);
    return &(get<I>(*shadowPage)[PAGE_OFFSET((uint64_t)address)]);
}
//#define ConcurrentShadowMemory ShadowMemory

#    if SHADOW_MEMORY_TEST
ShadowMemory<int> s;
ConcurrentShadowMemory <int> cs;
int main(){
    
for(int i = 0; i < 0xffff; i++){
    GetOrCreateShadowAddress<0>(s, 0x12345678)[0] = 1234;
    int j = GetOrCreateShadowAddress<0>(s, 0x12345678)[0];

    GetOrCreateShadowAddress<0>(cs, 0x12345678)[0] = 1234;
    j = GetOrCreateShadowAddress<0>(cs, 0x12345678)[0];
}
    return 0;
}
#    endif

#else // #if __cplusplus <= 199711L
// no tuple, atomic, and variadic template support.

struct nulltype {
};

template <class T1, class T2 = nulltype> struct tuple {
    T1 a[SHADOW_PAGE_SIZE];
    T2 b[SHADOW_PAGE_SIZE];
};

template <class T1, class T2 = nulltype> class ConcurrentShadowMemory {
    // All fwd declarations
    tuple<T1, T2> ***pageDirectory;
    // Given a address generated by the program, returns the corresponding shadow address
    // FLOORED to  SHADOW_PAGE_SIZE If the shadow page does not exist a new one is MMAPed
public:
    inline ConcurrentShadowMemory()
    {
        pageDirectory =
            (tuple<T1, T2> ***)mmap(0, LEVEL_1_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
                                    MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
        if (pageDirectory == MAP_FAILED) {
            perror("mmap pageDirectory");
            exit(-1);
        }
    }

    inline ~ConcurrentShadowMemory()
    {
        for (uint64_t i = 0; i < LEVEL_1_PAGE_TABLE_ENTRIES; i++) {
            tuple<T1, T2> **l1Page;
            if ((l1Page = __atomic_load_n(&pageDirectory[i], __ATOMIC_RELAXED)) != 0) {
                for (uint64_t j = 0; j < LEVEL_2_PAGE_TABLE_ENTRIES; j++) {
                    tuple<T1, T2> *l2Page;
                    if ((l2Page = __atomic_load_n(&l1Page[j], __ATOMIC_RELAXED)) != 0) {
                        delete l2Page;
                    }
                }
                if (0 != munmap(l1Page, LEVEL_2_PAGE_TABLE_SIZE)) {
                    perror("munmap pageDirectory");
                    exit(-1);
                }
            }
        }
        if (0 != munmap(pageDirectory, LEVEL_1_PAGE_TABLE_SIZE)) {
            perror("munmap pageDirectory");
            exit(-1);
        }
    }

    inline tuple<T1, T2> &
    GetOrCreateShadowBaseAddress(const size_t address)
    {
        tuple<T1, T2> ***l1Ptr = &pageDirectory[LEVEL_1_PAGE_TABLE_SLOT(address)];
        tuple<T1, T2> **v1;
        if ((v1 = __atomic_load_n(l1Ptr, __ATOMIC_ACQUIRE)) == 0) {
            tuple<T1, T2> **l1pg =
                (tuple<T1, T2> **)mmap(0, LEVEL_2_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
                                       MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
            if (l1pg == MAP_FAILED) {
                perror("mmap l1pg");
                exit(-1);
            }

            tuple<T1, T2> **nullVal = 0;
            if (!__atomic_compare_exchange_n(l1Ptr, &nullVal, l1pg, false,
                                             __ATOMIC_ACQ_REL, __ATOMIC_RELAXED)) {
                free(l1pg);
                v1 = __atomic_load_n(l1Ptr, __ATOMIC_ACQUIRE);
            } else {
                v1 = l1pg;
            }
        }
        tuple<T1, T2> **l2Ptr = &v1[LEVEL_2_PAGE_TABLE_SLOT(address)];
        tuple<T1, T2> *v2;
        if ((v2 = __atomic_load_n(l2Ptr, __ATOMIC_ACQUIRE)) == 0) {
            tuple<T1, T2> *l2pg = new tuple<T1, T2>;
            if (l2pg == MAP_FAILED) {
                perror("mmap l2pg");
                exit(-1);
            }
            tuple<T1, T2> *nullVal = 0;
            if (!__atomic_compare_exchange_n(l2Ptr, &nullVal, l2pg, false,
                                             __ATOMIC_ACQ_REL, __ATOMIC_RELAXED)) {
                delete l2pg;
                v2 = __atomic_load_n(l2Ptr, __ATOMIC_ACQUIRE);
            } else {
                v2 = l2pg;
            }
        }
        return (*v2);
    }

    inline T1 *
    GetOrCreateShadowBaseAddress_0(const size_t address)
    {
        return GetOrCreateShadowBaseAddress(address).a;
    }
    inline T2 *
    GetOrCreateShadowBaseAddress_1(const size_t address)
    {
        return GetOrCreateShadowBaseAddress(address).b;
    }
};

template <class T1, class T2 = nulltype> class ShadowMemory {
    // All fwd declarations
    tuple<T1, T2> ***pageDirectory;
    // Given a address generated by the program, returns the corresponding shadow address
    // FLOORED to  SHADOW_PAGE_SIZE If the shadow page does not exist a new one is MMAPed
public:
    inline ShadowMemory()
    {
        pageDirectory =
            (tuple<T1, T2> ***)mmap(0, LEVEL_1_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
                                    MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
        if (pageDirectory == MAP_FAILED) {
            perror("mmap pageDirectory");
            exit(-1);
        }
    }

    inline ~ShadowMemory()
    {
        for (uint64_t i = 0; i < LEVEL_1_PAGE_TABLE_ENTRIES; i++) {
            tuple<T1, T2> **l1Page;
            if ((l1Page = pageDirectory[i]) != 0) {
                for (uint64_t j = 0; j < LEVEL_2_PAGE_TABLE_ENTRIES; j++) {
                    tuple<T1, T2> *l2Page;
                    if ((l2Page = l1Page[j]) != 0) {
                        delete l2Page;
                    }
                }
                if (0 != munmap(l1Page, LEVEL_2_PAGE_TABLE_SIZE)) {
                    perror("munmap pageDirectory");
                    exit(-1);
                }
            }
        }
        if (0 != munmap(pageDirectory, LEVEL_1_PAGE_TABLE_SIZE)) {
            perror("munmap pageDirectory");
            exit(-1);
        }
    }

    inline tuple<T1, T2> &
    GetOrCreateShadowBaseAddress(const size_t address)
    {
        tuple<T1, T2> ***l1Ptr = &pageDirectory[LEVEL_1_PAGE_TABLE_SLOT(address)];
        tuple<T1, T2> **v1;
        if ((v1 = *l1Ptr) == 0) {
            tuple<T1, T2> **l1pg =
                (tuple<T1, T2> **)mmap(0, LEVEL_2_PAGE_TABLE_SIZE, PROT_WRITE | PROT_READ,
                                       MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
            if (l1pg == MAP_FAILED) {
                perror("mmap l1pg");
                exit(-1);
            }
            *l1Ptr = l1pg;
            v1 = l1pg;
        }
        tuple<T1, T2> **l2Ptr = &v1[LEVEL_2_PAGE_TABLE_SLOT(address)];
        tuple<T1, T2> *v2;
        if ((v2 = *l2Ptr) == 0) {
            tuple<T1, T2> *l2pg = new tuple<T1, T2>;
            if (l2pg == MAP_FAILED) {
                perror("mmap l2pg");
                exit(-1);
            }
            *l2Ptr = l2pg;
            v2 = l2pg;
        }
        return (*v2);
    }
};

#    define GetOrCreateShadowAddress_0(sm, addr) \
        &(sm.GetOrCreateShadowBaseAddress(addr).a[PAGE_OFFSET((uint64_t)addr)])

#    define GetOrCreateShadowAddress_1(sm, addr) \
        &(sm.GetOrCreateShadowBaseAddress(addr).b[PAGE_OFFSET((uint64_t)addr)])

#endif

#endif // __SHADOW_MEMORY__